{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coderdivine/UniqueWorkingModel/blob/main/WorkingModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9aILC1za4LB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define the dataset\n",
        "dataset = [\n",
        "    {\n",
        "        \"sentence\": \"I love to eat pizza\",\n",
        "        \"unique_words\": [\"I\", \"love\", \"to\", \"eat\", \"pizza\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The quick brown fox jumps over the lazy dog\",\n",
        "        \"unique_words\": [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy playing soccer with my friends\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"playing\", \"soccer\", \"with\", \"my\", \"friends\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The sun is shining brightly today\",\n",
        "        \"unique_words\": [\"The\", \"sun\", \"is\", \"shining\", \"brightly\", \"today\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to read books in my free time\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"read\", \"books\", \"in\", \"my\", \"free\", \"time\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"Cats are cute and fluffy\",\n",
        "        \"unique_words\": [\"Cats\", \"are\", \"cute\", \"and\", \"fluffy\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy going for long walks on the beach\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"going\", \"for\", \"long\", \"walks\", \"on\", \"the\", \"beach\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The sky is clear and the stars are twinkling\",\n",
        "        \"unique_words\": [\"The\", \"sky\", \"is\", \"clear\", \"and\", \"the\", \"stars\", \"are\", \"twinkling\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I prefer tea over coffee\",\n",
        "        \"unique_words\": [\"I\", \"prefer\", \"tea\", \"over\", \"coffee\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"Dogs are loyal companions\",\n",
        "        \"unique_words\": [\"Dogs\", \"are\", \"loyal\", \"companions\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I need to buy groceries for the week\",\n",
        "        \"unique_words\": [\"I\", \"need\", \"to\", \"buy\", \"groceries\", \"for\", \"the\", \"week\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The concert was fantastic\",\n",
        "        \"unique_words\": [\"The\", \"concert\", \"was\", \"fantastic\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy listening to music while I work\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"listening\", \"to\", \"music\", \"while\", \"work\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The movie kept me on the edge of my seat\",\n",
        "        \"unique_words\": [\"The\", \"movie\", \"kept\", \"me\", \"on\", \"the\", \"edge\", \"of\", \"my\", \"seat\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I love spending time with my family\",\n",
        "        \"unique_words\": [\"I\", \"love\", \"spending\", \"time\", \"with\", \"my\", \"family\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The rain is pouring outside\",\n",
        "        \"unique_words\": [\"The\", \"rain\", \"is\", \"pouring\", \"outside\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy going hiking in the mountains\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"going\", \"hiking\", \"in\", \"the\", \"mountains\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The book I'm reading is very interesting\",\n",
        "        \"unique_words\": [\"The\", \"book\", \"I'm\", \"reading\", \"is\", \"very\", \"interesting\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to bake cookies on the weekends\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"bake\", \"cookies\", \"on\", \"the\", \"weekends\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The city is bustling with activity\",\n",
        "        \"unique_words\": [\"The\", \"city\", \"is\", \"bustling\", \"with\", \"activity\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I need to finish my homework before tomorrow\",\n",
        "        \"unique_words\": [\"I\", \"need\", \"to\", \"finish\", \"my\", \"homework\", \"before\", \"tomorrow\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The flowers in the garden are blooming beautifully\",\n",
        "        \"unique_words\": [\"The\", \"flowers\", \"in\", \"the\", \"garden\", \"are\", \"blooming\", \"beautifully\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy watching movies in my free time\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"watching\", \"movies\", \"in\", \"my\", \"free\", \"time\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The beach is a great place to relax\",\n",
        "        \"unique_words\": [\"The\", \"beach\", \"is\", \"a\", \"great\", \"place\", \"to\", \"relax\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to go jogging in the morning\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"go\", \"jogging\", \"in\", \"the\", \"morning\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The painting is a masterpiece\",\n",
        "        \"unique_words\": [\"The\", \"painting\", \"is\", \"a\", \"masterpiece\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy trying new recipes\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"trying\", \"new\", \"recipes\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The river flows peacefully through the valley\",\n",
        "        \"unique_words\": [\"The\", \"river\", \"flows\", \"peacefully\", \"through\", \"the\", \"valley\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I need to catch up on my favorite TV show\",\n",
        "        \"unique_words\": [\"I\", \"need\", \"to\", \"catch\", \"up\", \"on\", \"my\", \"favorite\", \"TV\", \"show\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The computer crashed and I lost all my files\",\n",
        "        \"unique_words\": [\"The\", \"computer\", \"crashed\", \"and\", \"I\", \"lost\", \"all\", \"my\", \"files\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy going for a swim in the ocean\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"going\", \"for\", \"a\", \"swim\", \"in\", \"the\", \"ocean\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The sunsets here are breathtaking\",\n",
        "        \"unique_words\": [\"The\", \"sunsets\", \"here\", \"are\", \"breathtaking\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to go camping in the wilderness\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"go\", \"camping\", \"in\", \"the\", \"wilderness\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The traffic is heavy during rush hour\",\n",
        "        \"unique_words\": [\"The\", \"traffic\", \"is\", \"heavy\", \"during\", \"rush\", \"hour\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I need to buy a new pair of shoes\",\n",
        "        \"unique_words\": [\"I\", \"need\", \"to\", \"buy\", \"a\", \"new\", \"pair\", \"of\", \"shoes\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The coffee shop serves delicious pastries\",\n",
        "        \"unique_words\": [\"The\", \"coffee\", \"shop\", \"serves\", \"delicious\", \"pastries\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy solving puzzles in my spare time\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"solving\", \"puzzles\", \"in\", \"my\", \"spare\", \"time\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The snow is falling gently from the sky\",\n",
        "        \"unique_words\": [\"The\", \"snow\", \"is\", \"falling\", \"gently\", \"from\", \"the\", \"sky\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to go for long drives in the countryside\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"go\", \"for\", \"long\", \"drives\", \"in\", \"the\", \"countryside\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The building has a beautiful architecture\",\n",
        "        \"unique_words\": [\"The\", \"building\", \"has\", \"a\", \"beautiful\", \"architecture\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy playing board games with my family\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"playing\", \"board\", \"games\", \"with\", \"my\", \"family\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The forest is filled with tall trees and chirping birds\",\n",
        "        \"unique_words\": [\"The\", \"forest\", \"is\", \"filled\", \"with\", \"tall\", \"trees\", \"and\", \"chirping\", \"birds\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I need to fix the leaky faucet in the bathroom\",\n",
        "        \"unique_words\": [\"I\", \"need\", \"to\", \"fix\", \"the\", \"leaky\", \"faucet\", \"in\", \"the\", \"bathroom\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The park is a great place for a picnic\",\n",
        "        \"unique_words\": [\"The\", \"park\", \"is\", \"a\", \"great\", \"place\", \"for\", \"a\", \"picnic\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I enjoy taking photographs of nature\",\n",
        "        \"unique_words\": [\"I\", \"enjoy\", \"taking\", \"photographs\", \"of\", \"nature\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The clock is ticking, and I'm running out of time\",\n",
        "        \"unique_words\": [\"The\", \"clock\", \"is\", \"ticking\", \"and\", \"I'm\", \"running\", \"out\", \"of\", \"time\"]\n",
        "    }\n",
        "    # Add more sentences and their unique words as needed\n",
        "]\n",
        "\n",
        "dataset += [\n",
        "    {\n",
        "        \"sentence\": \"The cat sat on the mat\",\n",
        "        \"unique_words\": [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I saw a bird fly by\",\n",
        "        \"unique_words\": [\"I\", \"saw\", \"a\", \"bird\", \"fly\", \"by\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The dog ran after the ball\",\n",
        "        \"unique_words\": [\"The\", \"dog\", \"ran\", \"after\", \"the\", \"ball\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I like to read books\",\n",
        "        \"unique_words\": [\"I\", \"like\", \"to\", \"read\", \"books\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I went to the store to buy groceries\",\n",
        "        \"unique_words\": [\"I\", \"went\", \"to\", \"the\", \"store\", \"to\", \"buy\", \"groceries\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the park to play\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"park\", \"to\", \"play\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The sun is shining brightly\",\n",
        "        \"unique_words\": [\"The\", \"sun\", \"is\", \"shining\", \"brightly\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"The wind is blowing through the trees\",\n",
        "        \"unique_words\": [\"The\", \"wind\", \"is\", \"blowing\", \"through\", \"the\", \"trees\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm feeling happy today\",\n",
        "        \"unique_words\": [\"I'm\", \"feeling\", \"happy\", \"today\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to watch a movie tonight\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"watch\", \"a\", \"movie\", \"tonight\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the beach this weekend\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"beach\", \"this\", \"weekend\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to cook dinner tonight\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"cook\", \"dinner\", \"tonight\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the library to study\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"library\", \"to\", \"study\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the gym to work out\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"gym\", \"to\", \"work\", \"out\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the park to meet my friends\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"park\", \"to\", \"meet\", \"my\", \"friends\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the store to buy a new book\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"store\", \"to\", \"buy\", \"a\", \"new\", \"book\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the movies to see the new superhero movie\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"movies\", \"to\", \"see\", \"the\", \"new\", \"superhero\", \"movie\"]\n",
        "    },\n",
        "    {\n",
        "        \"sentence\": \"I'm going to the beach to relax and swim\",\n",
        "        \"unique_words\": [\"I'm\", \"going\", \"to\", \"the\", \"beach\", \"to\", \"relax\", \"and\", \"swim\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Save the dataset to a JSON file\n",
        "with open('dataset.json', 'w') as file:\n",
        "    json.dump(dataset, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMxDBB-RjnO-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the JSON dataset\n",
        "with open('dataset.json') as file:\n",
        "    dataset = json.load(file)\n",
        "    print(dataset)\n",
        "\n",
        "sentences = []\n",
        "unique_words = []\n",
        "\n",
        "# Extract sentences and unique words from the dataset\n",
        "for data in dataset:\n",
        "    sentence = data['sentence']\n",
        "    words = data['unique_words']\n",
        "    sentences.append(sentence)\n",
        "    unique_words.extend(words)\n",
        "\n",
        "# Convert unique words to lowercase\n",
        "unique_words = [word.lower() for word in unique_words]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Pad sequences to ensure they have the same length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length),\n",
        "    tf.keras.layers.LSTM(64),  # Replace Flatten with LSTM layer\n",
        "    tf.keras.layers.Dense(len(unique_words), activation='softmax')  # Use softmax activation for multiclass classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Use sparse categorical crossentropy for multiclass\n",
        "\n",
        "# Create the labels for word prediction\n",
        "word_labels = []\n",
        "for seq in sequences:\n",
        "    word_labels.extend(seq[1:])  # Use the next word as the label for each sequence\n",
        "\n",
        "# Truncate sequences and word_labels to have the same length\n",
        "truncated_sequences = padded_sequences[:, :-1]\n",
        "truncated_word_labels = word_labels[:truncated_sequences.shape[0]]\n",
        "\n",
        "# Adjust the length of truncated_sequences to match the expected input length\n",
        "truncated_sequences = pad_sequences(truncated_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Train the model\n",
        "print(truncated_sequences.shape, len(truncated_word_labels))\n",
        "model.fit(truncated_sequences, np.array(truncated_word_labels), epochs=15000)  # Use truncated_word_labels as the labels\n",
        "\n",
        "# Save the model\n",
        "model.save('word_prediction_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAK8rR7QWA8x"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('word_prediction_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwv1kdi6hSvZ"
      },
      "outputs": [],
      "source": [
        "# Preprocess the input sentence\n",
        "sentence = input(\"Enter text: \")\n",
        "sequence = tokenizer.texts_to_sequences([sentence])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = model.predict(padded_sequence)\n",
        "\n",
        "# Interpret the predictions\n",
        "threshold = 0.05  # Adjust the threshold as needed\n",
        "predicted_words = [tokenizer.index_word[i] for i, prediction in enumerate(predictions[0]) if prediction >= threshold]\n",
        "\n",
        "print('Predicted words:', predicted_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjIPOuM9E3u2Of0YogV9+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}